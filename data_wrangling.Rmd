---
title: "data_wrangling"
author: "stat group"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r libraries & env variables, include=FALSE, echo=FALSE}
options(scipen=999); `%notin%` <- Negate(`%in%`)
lib_list <- c('dplyr','ipumsr','ggplot2','dtplyr',
              'stringr','tidyr','rvest','chatgpt',
              'xml2','purrr','tibble','yahoofinancer','YRmisc',
              'caret','ModelMetrics','yardstick','car',
              'glmnet','tidyverse','randomForest','xgboost',
              'MASS','pROC')
lapply(lib_list, library, character.only=TRUE)
```

```{r parse_generalInfo}
#function to parse each worksheet and return a data frame
parse_generalInfo <- function(ws_ns, ws_node, index){
  ws_name <- xml_attr(ws_node, "Name", ws_ns)
  rows <- xml_find_all(ws_node, ".//ss:Row", ws_ns)
  #skip the first 6 rows
  rows <- rows[-(1:6)]
  #extract values per row
  row_data <- map(rows, function(row){
    cells <- xml_find_all(row, ".//ss:Cell", ws_ns)
    values <- map_chr(cells, ~ xml_text(xml_find_first(.x, ".//ss:Data", ws_ns)))
    return(values)})
  #skip empty worksheets
  if(length(row_data) == 0) return(tibble())  
  #pad and bind
  padded_data <- lapply(row_data, `length<-`, max(lengths(row_data)))
  parsed_df <- as.data.frame(do.call(rbind, padded_data), stringsAsFactors = FALSE) %>%
    mutate_all(~if_else(. == 'NA', NA, .)) %>%
    mutate(across(everything(), as.character))
  #coerce V2 to numeric
  v2_numeric <- suppressWarnings(as.numeric(parsed_df$V2))
  #find row indices where V2 is 1 and 51
  starts_raw <- which(v2_numeric == 1)
  ends <- which(v2_numeric == 51)
  #adjust start indices to be 4 rows earlier
  starts <- pmax(starts_raw - 4, 1)  # Ensure we don't go below row 1
  #build valid (non-overlapping) start:end ranges
  ranges <- list()
  used_ends <- c()
  for (i in seq_along(starts)){
    start <- starts[i]
    end_candidates <- ends[ends > starts_raw[i]]  # Ensure end is after raw "1"
    if (length(end_candidates) > 0){
      end <- end_candidates[1]
      if (!(end %in% used_ends)){
        ranges <- append(ranges, list(start:end))
        used_ends <- c(used_ends, end)}}}
  #flatten ranges into a vector of unique row indices to shift
  rows_to_shift <- unique(unlist(ranges))
  #shift V2–V16 left into V1–V15, set V16 to NA
  for (i in 1:15){parsed_df[rows_to_shift, paste0("V", i)] <- parsed_df[rows_to_shift, paste0("V", i + 1)]}
  parsed_df[rows_to_shift, "V16"] <- NA
  #data clean-up & manipulation
  parsed_df <- parsed_df %>%
    mutate(V8 = case_when(V7 %in% c(1996:2024) ~ V7,
                          .default = NA)) %>%
    #assign annual indicator
    fill(V8,.direction='down') %>%
    #create pivot version...based on annual values
    pivot_wider(id_cols = c(V1,V2),
                names_from = V8,
                values_from = V7,
                values_fn = list) %>%
    #collect relevant rows
    slice(2:110) %>%
    add_column(col_names=c('na1','gross_liab_prem','gross_prop_prem','gross_comined_prop_liab_prem','gross_other_lines_prem',
                           'gross_nonprop_reinsurance_prem','total_gross_prem','na2','net_liab_prem',
                           'net_prop_prem','net_comined_prop_liab_prem','net_other_lines_prem','net_nonprop_reinsurance_prem',
                           'total_net_prem','na3','net_underwriting_gain','net_investment_gain','total_other_income',
                           'dividends_to_policyholders','taxes_incurred','net_income','na4','total_assets','na5','prem_incourse','prem_deferred','prem_accured',
                           'total_liabilities','losses','loss_adjustment_expenses','unearned_prem','capital_paid_up','policyholders_surplus','na6','net_cash_from_operations','na7',
                           'total_adjusted_capital','control_level_capital','na8','na9','na10','bonds','stocks','real_estate_mortgage_loans','real_estate',
                           'cash_short-term_investment','contract_loans','derivatives','other_invested_assets','receivable_securities',
                           'securities_lending_reinvested_collateral_assets','agg_write-ins','cash_invested_assets','na11',
                           'affiliated_bonds','affiliated_preferred_stocks','affiliated_common_stocks','na12','affiliated_short-term_investments',
                           'affiliated_real_estate_mortgage_loans','other_affiliated','total_affiliated','total_in-parent_investment','na13',
                           'pct_affiliated_surplus','na14','net_unrealized_capital','divdends_to_stockholders',
                           'change_in-surplus_policyholders','na15','gross_liab_losses','gross_prop_losses','gross_comined_prop_liab_losses',
                           'na30','gross_other_lines_losses','gross_nonprop_reinsurance_losses','total_losses','na16',
                           'net_liab_losses','net_prop_losses','net_comined_prop_liab_losses','net_other_lines_losses',
                           'net_nonprop_reinsurance_losses','total_net_losses','na17','na18','earned_prem_percentage','incurred_losses',
                           'incurred_loss_expenses','other_incurred_underwriting_expenses','nwg',
                           'na19','na20','other_underwriting_expenses_to_net_prem','na21','imported_loss_ratio','na22','net_prem_to_policyholder_surplus','na23','na24',
                           '1yr_loss_development','na25','1yr_dev_loss_to_policyholders_surplus','na26','na27','2yr_loss_development',
                           'na28','na29','2yr_dev_loss_to_policyholders_surplus'),
               .before=2,.name_repair = 'minimal') %>%
    #collect relevant columns
    select(-c(V1,V2)) %>%
    #transpose data frame
    t() %>%
    as.data.frame() %>%
    #extract annual indicator from index
    rownames_to_column(var='index') %>%
    #convert data types from list...created by pivot operation
    mutate(across(where(is.list), ~ map_chr(.x, ~ paste(.x, collapse = ", "))))
  #assign column names
  colnames(parsed_df) <- parsed_df[1,]
  colnames(parsed_df)[1] <- 'Year'
  parsed_df <- parsed_df[-1,]
  #return final data frame
  return(parsed_df)
}
```

```{r parse_earnedPrem}
#function to parse each worksheet and return a data frame
parse_earnedPrem <- function(ws_ns, ws_node, index){
  ws_name <- xml_attr(ws_node, "Name", ws_ns)
  rows <- xml_find_all(ws_node, ".//ss:Row", ws_ns)
  #skip the first 6 rows
  #rows <- rows[-(1:6)]
  #extract values per row
  row_data <- map(rows, function(row){
    cells <- xml_find_all(row, ".//ss:Cell", ws_ns)
    values <- map_chr(cells, ~ xml_text(xml_find_first(.x, ".//ss:Data", ws_ns)))
    return(values)})
  #skip empty worksheets
  if(length(row_data) == 0) return(tibble())  
  #pad and bind
  padded_data <- lapply(row_data, `length<-`, max(lengths(row_data)))
  parsed_df <- as.data.frame(do.call(rbind, padded_data), stringsAsFactors = FALSE) %>%
    mutate_all(~if_else(. == 'NA', NA, .)) %>%
    mutate(across(everything(), as.character)) 
  title_str <- parsed_df[1,'V2']
  parsed_df <- parsed_df %>%
    slice(-c(1:7)) %>%
    select(V2,V3,V8,V9,V10,V11) %>%
    rename(lob_index=V2,lob=V3,net_written_prem=V8,prior_unearned_prem=V9,unearned_prem=V10,earned_prem=V11) %>%
    mutate(suppressWarnings(round(as.numeric(lob_index), 1)),
           across(c(net_written_prem,prior_unearned_prem,unearned_prem,earned_prem),~ suppressWarnings(as.numeric(.x))),
           year=as.integer(sub('.*December 31, (.*?) OF.*','\\1',title_str))) %>%
    filter(lob=='TOTALS') %>%
    select(year,earned_prem)
  parsed_df <- parsed_df %>%
    mutate(year=year-row_number()+1)
  return(parsed_df)
}
```

```{r parse_paidExpenses}
#function to parse each worksheet and return a data frame
parse_paidExpenses <- function(ws_ns, ws_node, index){
  ws_name <- xml_attr(ws_node, "Name", ws_ns)
  rows <- xml_find_all(ws_node, ".//ss:Row", ws_ns)
  #skip the first 6 rows
  #rows <- rows[-(1:6)]
  #extract values per row
  row_data <- map(rows, function(row){
    cells <- xml_find_all(row, ".//ss:Cell", ws_ns)
    values <- map_chr(cells, ~ xml_text(xml_find_first(.x, ".//ss:Data", ws_ns)))
    return(values)})
  #skip empty worksheets
  if(length(row_data) == 0) return(tibble())  
  #pad and bind
  padded_data <- lapply(row_data, `length<-`, max(lengths(row_data)))
  parsed_df <- as.data.frame(do.call(rbind, padded_data), stringsAsFactors = FALSE) %>%
    mutate_all(~if_else(. == 'NA', NA, .)) %>%
    mutate(across(everything(), as.character)) 
  title_str <- parsed_df[1,'V1']
  parsed_df <- parsed_df %>%
    slice(-c(1:6)) %>%
    select(V2,V3,V7,V8,V9,V11) %>%
    rename(label=V2,description=V3,lae=V7,underwriting_exp=V8,investment_exp=V9,total_exp=V11) %>%
    mutate(across(c(lae,underwriting_exp,investment_exp,total_exp),~ suppressWarnings(as.numeric(.x))),
           year=as.integer(sub('.*December 31, (.*?) OF.*','\\1',title_str))) %>%
    filter(label=='TOTAL EXPENSES PAID (Lines 25 - 26 + 27 - 28 + 29)') %>%
    select(year,lae,underwriting_exp,investment_exp,total_exp)%>%
    mutate(year=year-row_number()+1)
  return(parsed_df)
}
```

```{r parse_stockPrice}
#get historical market data
parse_stockPrice <- function(tick_df){
  tickers_df <- data.frame()
  for(tick in c(1:nrow(tick_df))){
    #initialize ticker
    temp_tick <- Ticker$new(tick_df[tick,'ticker'])
    #import historic data
    temp_price_df <- temp_tick$get_history(start = '1996-01-01', end = '2024-12-31', interval = '1d')[,c(1,7)]
    #aggregate information annually
    temp_price_df <- temp_price_df %>%
      group_by(Year = lubridate::year(date)) %>%
      mutate(adj_close = map_dbl(adj_close, ~ pluck(.x, 1, .default = NA_real_))) %>%
      summarize(across(adj_close, ~last(.))) %>%
      mutate(Company = tick_df[tick,'name'])
    #add ticker information to output data frame
    tickers_df <- rbind(tickers_df,temp_price_df)
  }
  #collect stock price information
  tickers_df <- tickers_df %>%
    #assign proper column formats
    mutate(stock_price = map_dbl(adj_close, ~ if(length(.x) > 0){.x[[1]]}else{NA_real_}),
           Year = as.character(Year)) %>%
    #drop adj_close column
    select(-adj_close)
  return(tickers_df)
}
```

```{r data_import}
#function to perform all data import operations
data_import<- function(input.df,input.indicators){
  #initialize output data frame
  orgs_df <- data.frame()
  #collect information from financial statements
  for(org in c(1:nrow(input.df))){
    #import .xml file
    temp_general_doc <- read_xml(paste0("../../research/fin_statements/general/",input.df[org,'name'],".xml"))
    temp_ep_doc <- read_xml(paste0("../../research/fin_statements/earned_prem/",input.df[org,'name'],".xml"))
    temp_paidExp_doc <- read_xml(paste0("../../research/fin_statements/paid_expenses/",input.df[org,'name'],".xml"))
    #initialize node information from file
    temp_general_ns <- xml_ns(temp_general_doc)
    temp_ep_ns <- xml_ns(temp_ep_doc)
    temp_paidExp_ns <- xml_ns(temp_paidExp_doc)
    #extract information about all 'Worksheet' nodes...annual information
    temp_general_wks <- xml_find_all(temp_general_doc, ".//ss:Worksheet", temp_general_ns)
    temp_ep_wks <- xml_find_all(temp_ep_doc, ".//ss:Worksheet", temp_ep_ns)
    temp_paidExp_wks <- xml_find_all(temp_paidExp_doc, ".//ss:Worksheet", temp_paidExp_ns)
    #apply parsing algorithm and assign company indicator
    temp_org_df <- parse_generalInfo(temp_general_ns,temp_general_wks,seq_along(temp_general_wks)) %>% 
      merge(y=parse_earnedPrem(temp_ep_ns,temp_ep_wks,seq_along(temp_ep_wks)),by.x='Year',by.y='year') %>%
      merge(y=parse_paidExpenses(temp_paidExp_ns,temp_paidExp_wks,seq_along(temp_paidExp_wks)),by.x='Year',by.y='year') %>%
      add_column(Company=input.df[org,'name'],.before=2,.name_repair = 'minimal')
    #add company information to output data frame
    orgs_df <- rbind(orgs_df,temp_org_df)}
  #include target variable
  org_stockPrice <- parse_stockPrice(input.df)
  orgs_df <- merge(x=orgs_df,y=org_stockPrice,by=c('Year','Company'),all.x=TRUE)
  # include economic indicators
  econ_indicators <- xd.fred(input.indicators,'1996-01-01','2024-12-31') %>%
    fill('GDPC1',.direction='updown') %>%
    fill('CPALTT01USM657N',.direction='down') %>%
    mutate(Year = lubridate::year(observation_date)) %>%
    group_by(Year) %>%
    summarize(across(where(is.numeric), ~ tail(na.omit(.), 1))) %>%
    mutate(Year = as.character(Year))
  orgs_df <- merge(x=orgs_df,y=econ_indicators,by='Year',all.x=TRUE)
  #return output data frame
  orgs_df <- orgs_df %>%
    mutate_all(~if_else(. == 'NA', NA, .)) %>%
    select(-contains('na'))
  return(orgs_df)
}
```

```{r initialize parameters}
#reference: https://www.reinsurancene.ws/top-100-u-s-property-casualty-insurance-companies/
# sample_list <- c('State Farm','Berkshire Hathaway','Progressive','Allstate','Liberty Mutual',
#                  'Travelers','USAA','Chubb','Nationwide','Farmers Alliance',
#                  'American Family Ins','AIG','Fairfax Financial','Auto-Owners Ins','Tokio Marine',
#                  'Erie Ins','Berkley Ins','Hartford Fire Ins','Everest Re','CNA')

#define company names
pc_names <- c('Berkshire Hathaway A','Berkshire Hathaway B','Progressive','Allstate','Hanover Ins. Group',
              'Travelers','Zurich Ins. Group','Chubb','Fairfax Financial','Selective Ins. Group',
              'Allianz SE','AIG','Kemper Corp.','QBE Ins. Group','Tokio Marine',
              'Assurant Inc.','Berkley Ins.','Hartford Fire Ins.','Everest Re.','CNA')

#define company tickers...based on yahoo finance
pc_tickers <- c('brk-a','brk-b','prg','all','thg',
                'trv','zurn.sw','cb','ffh.to','sigi',
                'alv.de','aig','kmpr','qbe.ax','8766.t',
                'aiz','wrb','hig','eg','cna')

#define company local currency
pc_currency <- c('USD','USD','USD','USD','USD',
                 'USD','CHF','USD','CAD','USD',
                 'EUR','USD','USD','AUD','JPY',
                 'USD','USD','USD','USD','USD')

#initialize input data frame for data import
pcInput_df <- data.frame(name=pc_names,
                         ticker=pc_tickers,
                         currency=pc_currency) 

#real GDP, employment no, CPI, retail sales, money supply
pc_econIndicators <- c('GDPC1','PAYEMS','CPALTT01USM657N','MRTSSM44000USS','M2REAL')
```

```{r define base data}
#compile data...
  #1. financial statements
  #2. economic indicators
  #3. stock price
base_df <- data_import(pcInput_df,pc_econIndicators)
```

```{r define columns from literature review,rows.print=50}
eda_df <- base_df %>%
  mutate(across(c(net_income,policyholders_surplus,total_assets,total_liabilities,net_cash_from_operations,
                  unearned_prem,earned_prem,net_investment_gain,prem_deferred,prem_incourse,total_net_prem,
                  losses,lae,net_underwriting_gain,`change_in-surplus_policyholders`,
                  `1yr_loss_development`),
                ~suppressWarnings(as.numeric(.x))),
         return_on_surplus=net_income/policyholders_surplus,
         return_on_assets=net_income/total_assets,
         liab_to_liquidAssets=(total_liabilities-prem_deferred)/(net_cash_from_operations+ #cash
                                                     earned_prem+ #cash
                                                     unearned_prem+ #accounts receivable
                                                     net_investment_gain), #cash
         inCoursePrem_to_surplus=prem_incourse/policyholders_surplus,
         writtenPrem_to_surplus=total_net_prem/policyholders_surplus,
         lossReserves_to_surplus=(losses+lae)/policyholders_surplus,
         debt_to_equity=total_liabilities/policyholders_surplus,
         loss_ratio=(losses+lae)/earned_prem,
         combined_ratio=(losses+lae+underwriting_exp)/earned_prem,
         operating_ratio=(losses+lae+total_exp-net_investment_gain)/earned_prem,
         uw_margin=net_underwriting_gain/total_net_prem,
         investment_income=net_investment_gain/earned_prem,
         investment_yield=net_investment_gain/net_cash_from_operations,
         return_on_investments=net_investment_gain/policyholders_surplus,
         surplus_growth=`change_in-surplus_policyholders`/(policyholders_surplus-`change_in-surplus_policyholders`),
         assets_to_surplus=total_assets/policyholders_surplus,
         oneYRDev_to_surplus=`1yr_loss_development`/policyholders_surplus,
         Company=as.factor(Company),Year=as.factor(Year),
         discrete_lossRatio=case_when(loss_ratio <= 1 ~ 1,
                                      loss_ratio > 1 & loss_ratio <= 2 ~ 2,
                                      .default = 3)) %>% 
  select(Year,Company,return_on_surplus,return_on_assets,liab_to_liquidAssets,inCoursePrem_to_surplus,
         writtenPrem_to_surplus,lossReserves_to_surplus,debt_to_equity,loss_ratio,uw_margin,investment_income,
         investment_yield,return_on_investments,stock_price,surplus_growth,oneYRDev_to_surplus,
         GDPC1,PAYEMS,CPALTT01USM657N,MRTSSM44000USS,M2REAL,combined_ratio,operating_ratio,discrete_lossRatio) %>%
  na.omit()
```

```{r loss ratio ts by company,rows.print=20,fig.width=20}
eda_df %>%
  mutate(date=as.Date(paste0(Year,'/12/31'))) %>%
  ggplot(aes(x=date,y=discrete_lossRatio)) +
  geom_line() + 
  theme_minimal() +
  facet_wrap(~as.factor(Company)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) 
```

```{r multi-collinearity analysis}
#create correlation threshold
threshold <- 0.7
#create correlation matrix
ref_cor <- data.frame(cor(eda_df[,names(eda_df)[sapply(eda_df,is.numeric)]]))
#analyze correlation matrix...filter by threshold value
cor_data <- data.frame(names = names(ref_cor)) #create a new table for filtered values
for (col in names(ref_cor)) { #evaluate each column in the correlation matrix
  cor_vec <- ref_cor[, col] #collect individual column correlation scores
  cor_data <- cor_data %>%
    mutate(!!col := case_when( #conditional definition...based on threshold value
                      abs(cor_vec) >= threshold ~ cor_vec, #when value meets threshold
                      abs(cor_vec) < threshold ~ NA, #when value violates threshold
                    ))}
#display filtered correlation matrix
cor_data  
#correlated combinations
  #return_on_assets->return_on_surplus
  #liab_to_liquidAssets->loss_ratio->operating_ratio->combined_ratio
  #lossReserves_to_surplus->debt_to_equity
  #GDPC1->PAYEMS->MRTSSM44000USS->M2REAL
```

```{r lm setup}
#set split seed
set.seed(100)
#define correlated columns
correlated_cols <- c('operating_ratio','combined_ratio','liab_to_liquidAssets','debt_to_equity','M2REAL',
                     'PAYEMS','MRTSSM44000USS','return_on_assets','GDPC1','CPALTT01USM657N','Company')
#define split criteria
lm_trainIndices <- createDataPartition(eda_df$loss_ratio, p = 0.75, list = FALSE)
#perform split operation...train & test samples
lm_train <- eda_df[lm_trainIndices, names(eda_df) %notin% correlated_cols]
lm_test  <- eda_df[-lm_trainIndices, names(eda_df) %notin% correlated_cols]
#separate samples into predictors (x) & target (y)
lm_XTrain <- lm_train %>% dplyr::select(-c(loss_ratio)); lm_YTrain <- lm_train$loss_ratio
lm_XTest <- lm_test %>% dplyr::select(-c(loss_ratio)); lm_YTest <- lm_test$loss_ratio
```

```{r lm model fit}
#set modeling seed
set.seed(100)
#fit model object...with all columns
mlr.model <- lm(loss_ratio~.,data=lm_train)
#perform predictions with fitted model...using test sample
mlr.pred <- predict(mlr.model,newdata=lm_XTest)
#compute rmse & r-squared metrics...using test sample
print(paste0('Test MLR RMSE: ',round(as.numeric(ModelMetrics::rmse(actual=lm_YTest,pred=mlr.pred)),3)))
print(paste0('Test MLR R-Squared: ',round(as.numeric(rsq(data.frame(actual=lm_YTest,pred=mlr.pred),truth=actual,estimate=pred)[1,'.estimate']),2)*100,'%'))
```
```{r lm diagnostics}
#homoscedasticity check
variance.plot <- function(model){ #plot fitted values vs residuals 
  #save the residuals from the model
  residuals <- residuals(model)
  #create the variance plot
  plot(fitted(model), residuals, xlab = 'fitted values', ylab = 'residuals', 
       main = 'variance plot for homoscedasticity')
  
  # Add a smooth line to the plot
  lines(lowess(fitted(model), residuals), col = 'red')
}
#display variance plot
variance.plot(mlr.model)
#display q-q plots
qqnorm(residuals(mlr.model),
       main='normal q-q plot',
       xlab='theoretical quantiles',
       ylab='sample quantiles')
qqline(residuals(mlr.model))
#display cook's distance metrics
plot(mlr.model,4)
#abnormal activity:
  #440: Zurich's 2017 investment activities...catastrophe payouts
  #566 & 567: Berkshire Hathaway investment activities...relatively better
#display vif metrics
vif(mlr.model) %>%
  as.data.frame() %>%
  mutate(feature=rownames(vif(mlr.model)),
         feature = fct_reorder(feature, GVIF, .desc = FALSE)) %>%
  ggplot() +
  geom_col(aes(y=feature,x=GVIF)) +
  labs(title='variance inflation factors',
       subtitle='using gvif',
       x='gvif',
       y='feature')+
  theme_minimal()+
  theme(plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5)) 
```
```{r lasso setup}
#set seed
set.seed(100)
#define split criteria
lasso_trainIndices <- createDataPartition(eda_df$loss_ratio, p = 0.75, list = FALSE)
#perform split operation...train & test samples
lasso_train <- eda_df[lasso_trainIndices, names(eda_df) %notin% correlated_cols]
lasso_test  <- eda_df[-lasso_trainIndices, names(eda_df) %notin% correlated_cols]
#separate samples into predictors (x) & target (y)
lasso_XTrain <- lasso_train %>% dplyr::select(-c(loss_ratio)) %>% as.matrix(); lasso_YTrain <- lasso_train$loss_ratio
lasso_XTest <- lasso_test %>% dplyr::select(-c(loss_ratio)) %>% as.matrix(); lasso_YTest <- lasso_test$loss_ratio
```

```{r lasso base model fit}
#set seed
set.seed(100)
baseLasso.model <- suppressWarnings(cv.glmnet(lasso_XTrain,lasso_YTrain,alpha=1))
#extract best lambda
plot(baseLasso.model,main='')
title(main = paste0('L1 error vs MSE with optimal log(lambda) ~ ',log(baseLasso.model$lambda.min)), line = 2.5)
```

```{r lasso final model fit}
#set seed
set.seed(100)
finalLasso.model <- suppressWarnings(glmnet(lasso_XTrain,lasso_YTrain,alpha=1,lambda=baseLasso.model$lambda.min))
lasso.pred <- suppressWarnings(predict(finalLasso.model,newx=lasso_XTest))
```

```{r lasso diagnostic, rows.print=25}
#perform feature selection
selected_features <- as.data.frame(coef(finalLasso.model)[which(coef(finalLasso.model) != 0), ]) %>% 
  rename(coef=`coef(finalLasso.model)[which(coef(finalLasso.model) != 0), ]`) 
selected_features <- selected_features %>%
  mutate(feature=rownames(selected_features),
         feature = fct_reorder(feature, coef, .desc = FALSE)) %>% 
  filter(feature!='oneYRDev_to_surplus') #filter outlier importance...to view other relative importance relationships
selected_features %>%
  ggplot(aes(y=feature,x=coef))+
    geom_col() +
  theme_minimal() + 
  labs(title = paste0('lasso feature selection for model with: \n \t rmse: ',
                      round(as.numeric(ModelMetrics::rmse(actual=lasso_YTest,pred=lasso.pred)),2),
                      '\n \t r-squared: ',
                      round(as.numeric(rsq(data.frame(actual=lasso_YTest,pred=lasso.pred%>%as.data.frame()%>%rename(pred=s0)),
                                           truth=actual,
                                           estimate=pred)[1,'.estimate']),2)),
       x='lasso coefficient')
```
```{r rf setup}
#set seed
set.seed(100)
#define split criteria
rf_trainIndices <- createDataPartition(eda_df$loss_ratio, p = 0.75, list = FALSE)
#perform split operation...train & test samples
rf_train <- eda_df[rf_trainIndices, names(eda_df) %notin% correlated_cols]
rf_test  <- eda_df[-rf_trainIndices, names(eda_df) %notin% correlated_cols]
#separate samples into predictors (x) & target (y)
rf_XTrain <- rf_train %>% dplyr::select(-c(loss_ratio)); rf_YTrain <- rf_train$loss_ratio
rf_XTest <- rf_test %>% dplyr::select(-c(loss_ratio)); rf_YTest <- rf_test$loss_ratio
```

```{r rf model fit}
set.seed(100)
rf.model <- randomForest(rf_XTrain, rf_YTrain, importance = TRUE, ntree = 500)
rf.pred <- predict(rf.model,newx=rf_XTest)
```

```{r rf diagnostic}
#feature importance
importance_df <- as.data.frame(importance(rf.model))
importance_df$feature <- rownames(importance_df)
#plot importance (IncMSE used here)
importance_df %>%
  arrange(desc(`%IncMSE`)) %>%
  mutate(feature = fct_reorder(feature, `%IncMSE`)) %>%
  ggplot(aes(x = `%IncMSE`, y = feature)) +
    geom_col() +
    labs(title = paste0("random forest feature importance with: \n \t rmse: ",
                        round(sqrt(tail(rf.model$mse,1)),2),
                        '\n \t r-squared: ',
                        round(tail(rf.model$rsq,1),2)),
         x = "% Increase in MSE", y = "")
```

```{r xgb setup}
#set seed
set.seed(100)
#define split criteria
xgb_trainIndices <- createDataPartition(eda_df$loss_ratio, p = 0.75, list = FALSE)
#perform split operation...train & test samples
xgb_train <- eda_df[xgb_trainIndices, names(eda_df) %notin% correlated_cols] %>% mutate(Year=as.numeric(Year))
xgb_test  <- eda_df[-xgb_trainIndices, names(eda_df) %notin% correlated_cols] %>% mutate(Year=as.numeric(Year))
#separate samples into predictors (x) & target (y)
xgb_XTrain <- xgb_train %>% dplyr::select(-c(loss_ratio)) %>% as.matrix(); xgb_YTrain <- xgb_train$loss_ratio
xgb_XTest <- xgb_test %>% dplyr::select(-c(loss_ratio)) %>% as.matrix(); xgb_YTest <- xgb_test$loss_ratio
```

```{r xgb model fit}
#fit xgboost regression model
set.seed(100)
xgb_model <- xgboost(data = xgb.DMatrix(data = xgb_XTrain, label = xgb_YTrain), 
                     objective = "reg:squarederror", 
                     nrounds = 100, 
                     verbose = 0)
```

```{r xgb diagnostic}
y_pred <- predict(xgb_model, newdata = xgb_XTest)
#calculate r-squared
rss <- sum((xgb_YTest - y_pred)^2)
tss <- sum((xgb_YTest - mean(xgb_YTest))^2)
rsq <- 1 - rss/tss
#plot feature importance
xgb.plot.importance(xgb.importance(model = xgb_model), top_n = 10, 
                    rel_to_first = TRUE, 
                    main = paste0("xgboost feature importance with: \n \t rmse: ",
                                  round(ModelMetrics::rmse(xgb_YTest,y_pred),2),
                                  '\n \t r-squared: ',
                                  round(rsq, 2)))
```
```{r lda setup}
#set split seed
set.seed(100)
#define correlated columns
correlated_cols <- c('operating_ratio','combined_ratio','liab_to_liquidAssets','debt_to_equity','M2REAL','Year',
                     'PAYEMS','MRTSSM44000USS','return_on_assets','GDPC1','CPALTT01USM657N','Company','loss_ratio','oneYRDev_to_surplus')
#define split criteria
lda_trainIndices <- createDataPartition(eda_df$discrete_lossRatio, p = 0.75, list = FALSE)
#perform split operation...train & test samples
lda_train <- eda_df[lda_trainIndices, names(eda_df) %notin% correlated_cols]
lda_test  <- eda_df[-lda_trainIndices, names(eda_df) %notin% correlated_cols]
#separate samples into predictors (x) & target (y)
lda_XTrain <- lda_train %>% dplyr::select(-c(discrete_lossRatio)); lda_YTrain <- lda_train$discrete_lossRatio %>% as.factor()
lda_XTest <- lda_test %>% dplyr::select(-c(discrete_lossRatio)); lda_YTest <- lda_test$discrete_lossRatio %>% as.factor()
```

```{r}
lda.model <- lda(discrete_lossRatio ~ ., data = lda_train)

# View model summary
print(lda.model)

# Predict on training data
lda.pred <- predict(lda.model, lda_XTest)

# Confusion matrix
lda.table <- table(Predicted = lda.pred$class, Actual = lda_YTest)

lda_errorRate <- mean(lda.pred$class != lda_YTest)
cat("\nlda accuracy:", 1-round(lda_errorRate, 4), "\n")
```

```{r lda ld1 feature importance}
lda_coeffs <- as.data.frame(lda.model$scaling)
lda_coeffs$Feature <- rownames(lda_coeffs)

lda_coeffs <- lda_coeffs %>%
  mutate(Importance = abs(LD1))

lda_coeffs %>%
  arrange(desc(Importance)) %>%
  mutate(Feature = fct_reorder(Feature, Importance)) %>%
  ggplot(aes(x = Importance, y = Feature)) +
  geom_col(fill = "steelblue") +
  labs(title = "LDA Feature Importance", x = "Importance", y = "")
```

```{r lda all lds feature importance}
lda_coeffs <- as.data.frame(lda.model$scaling)
lda_coeffs$Feature <- rownames(lda_coeffs)

lda_coeffs <- lda_coeffs %>%
  mutate(Importance = sqrt(rowSums(across(starts_with("LD"))^2)))

lda_coeffs %>%
  arrange(desc(Importance)) %>%
  mutate(Feature = fct_reorder(Feature, Importance)) %>%
  ggplot(aes(x = Importance, y = Feature)) +
  geom_col(fill = "steelblue") +
  labs(title = "LDA Feature Importance", x = "Importance", y = "")
```

```{r lda diagnostic}
# Plot LDs
data.frame(lda.pred$x, risk_level = lda_YTest) %>%
  ggplot(aes(x = LD1, y = LD2, color = risk_level)) +
    geom_point() +
    labs(title = "LDA: Linear Discriminants")
# Actual class labels (factor)
actual <- lda_test$discrete_lossRatio

# Predicted posterior probabilities
posterior <- as.data.frame(lda.pred$posterior)

# Loop over each class
auc_list <- list()
roc_list <- list()

for (cls in colnames(posterior)) {
  # One-vs-rest: binary actual (1 if class = cls, 0 otherwise)
  actual_binary <- as.numeric(actual == cls)
  roc_obj <- suppressMessages(roc(actual_binary, posterior[[cls]]))

  auc_list[[cls]] <- auc(roc_obj)
  roc_list[[cls]] <- roc_obj
}

# Display AUCs
print(auc_list)
#
# Plot all ROC curves
plot(roc_list[[1]], col = 1, main = "One-vs-Rest ROC Curves (LDA)", lwd = 2)
for (i in 2:length(roc_list)) {
  plot(roc_list[[i]], add = TRUE, col = i, lwd = 2)
}
legend("bottomright", legend = names(roc_list), col = 1:length(roc_list), lwd = 2)
```